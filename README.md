# ğŸ“Œ Pipeline ETL con IntegraciÃ³n de Datos

## ğŸ“¢ DescripciÃ³n
Este proyecto implementa un pipeline ETL (Extract, Transform, Load) para extraer datos de varias fuentes, transformarlos y cargarlos en una base de datos centralizada. El objetivo es construir un flujo de datos automatizado y escalable utilizando herramientas de procesamiento de datos.

## ğŸ¯ Objetivos
âœ… Extraer datos de diferentes fuentes (APIs, bases de datos SQL, archivos CSV/JSON)
âœ… Limpiar, transformar y validar los datos para asegurar calidad y coherencia
âœ… Cargar los datos en un Data Warehouse o base de datos optimizada
âœ… Automatizar el proceso utilizando Apache Airflow
âœ… Implementar validaciones de calidad con Great Expectations

## ğŸ› ï¸ Tech Stack
- **Lenguaje**: Python
- **OrquestaciÃ³n**: Apache Airflow
- **Almacenamiento**: PostgreSQL, Azure Data Lake
- **Procesamiento**: Pandas, SQLAlchemy
- **ValidaciÃ³n**: Great Expectations
- **Herramientas auxiliares**: Docker, GitHub Actions

## ğŸ—ï¸ Arquitectura del Proyecto
1. **ExtracciÃ³n (Extract)**
   - ObtenciÃ³n de datos desde APIs REST
   - Lectura de datos desde archivos CSV/JSON
   - ConexiÃ³n a bases de datos SQL
2. **TransformaciÃ³n (Transform)**
   - Limpieza de datos con Pandas
   - Validaciones de tipos y valores nulos
   - AgregaciÃ³n y filtrado de datos relevantes
3. **Carga (Load)**
   - InserciÃ³n de datos en PostgreSQL
   - Almacenamiento en Azure Data Lake
   - CreaciÃ³n de tablas optimizadas para consultas
4. **AutomatizaciÃ³n**
   - DefiniciÃ³n de DAGs en Apache Airflow
   - MonitorizaciÃ³n de tareas y manejo de errores
5. **ValidaciÃ³n de Datos**
   - Uso de Great Expectations para garantizar calidad de datos
   - Pruebas unitarias con PyTest

## ğŸš€ Desarrollo del Proyecto
### 1ï¸âƒ£ Configurar el entorno
- InstalaciÃ³n de dependencias con `pip install -r requirements.txt`
- ConfiguraciÃ³n de variables de entorno para conexiones
- CreaciÃ³n de base de datos en PostgreSQL

### 2ï¸âƒ£ Implementar el pipeline ETL
- Desarrollo de scripts de extracciÃ³n en Python
- Transformaciones con Pandas
- InserciÃ³n de datos en PostgreSQL
- CreaciÃ³n de DAGs en Apache Airflow

### 3ï¸âƒ£ ValidaciÃ³n y pruebas
- Pruebas unitarias con PyTest
- Validaciones con Great Expectations
- EvaluaciÃ³n de calidad de datos

### 4ï¸âƒ£ DocumentaciÃ³n y despliegue
- CreaciÃ³n de README detallado
- PublicaciÃ³n en GitHub
- ConfiguraciÃ³n de CI/CD con GitHub Actions

## ğŸ“‚ Estructura del Proyecto
```
project-root/
â”‚â”€â”€ etl_pipeline/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â”œâ”€â”€ validations.py
â”‚   â”œâ”€â”€ dag_airflow.py
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.csv
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ test_transform.py
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

## ğŸ“Š Resultados Esperados
ğŸ”¹ Pipeline ETL funcionando con datos extraÃ­dos, transformados y cargados
ğŸ”¹ Datos limpios y estructurados listos para anÃ¡lisis
ğŸ”¹ AutomatizaciÃ³n con Airflow para ejecutar tareas periÃ³dicamente
ğŸ”¹ Validaciones que aseguran la calidad y fiabilidad del dataset

## ğŸ’¡ Siguientes Pasos
ğŸ”¹ Optimizar tiempos de procesamiento con Apache Spark
ğŸ”¹ Implementar una API para consulta de datos
ğŸ”¹ Incluir dashboard de monitoreo con Grafana

---
ğŸ› ï¸ **Autor:** Carolina Vargas Arce  
ğŸ“… **Fecha de CreaciÃ³n:** Marzo 2025  
ğŸ”— **Repositorio GitHub:** [[Enlace al repo](https://github.com/carolinavarce/carolinavarce/blob/main/README.md#-siguientes-pasos)]

