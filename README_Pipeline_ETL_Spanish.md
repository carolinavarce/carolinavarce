# ğŸ“Œ Pipeline ETL con IntegraciÃ³n de Datos y ExtracciÃ³n desde API

## ğŸ“¢ DescripciÃ³n
Este proyecto implementa un pipeline ETL (Extract, Transform, Load) para extraer datos de mÃºltiples fuentes (archivos locales y API REST), transformarlos, validarlos y cargarlos en una base de datos centralizada. El objetivo es construir un flujo de datos automatizado y escalable utilizando herramientas modernas de orquestaciÃ³n, almacenamiento y validaciÃ³n.

## ğŸ¯ Objetivos

âœ… Extraer datos desde:
- APIs REST
- Archivos CSV/JSON
- Bases de datos SQL

âœ… Transformar y validar los datos:
- Limpieza con Pandas
- ValidaciÃ³n de tipos, nulos y duplicados
- PreparaciÃ³n para carga masiva

âœ… Cargar datos en:
- Base de datos PostgreSQL (estructurada)
- Archivos transformados (JSON/CSV limpios)

âœ… Automatizar el flujo con Apache Airflow

âœ… Validar la calidad del dato con Great Expectations

---

## ğŸ› ï¸ Tech Stack

- **Lenguaje**: Python 3
- **Procesamiento**: Pandas, SQLAlchemy, requests
- **Almacenamiento**: PostgreSQL, CSV
- **ValidaciÃ³n**: Great Expectations
- **OrquestaciÃ³n**: Apache Airflow
- **Entorno virtual**: airflow_venv
- **Control de versiones**: GitHub
- *(Opcional)*: Docker, Azure Data Lake

---

## ğŸ—ï¸ Arquitectura del Proyecto

1. **ExtracciÃ³n (Extract)**
   - Lectura de archivos CSV (`empleados.csv`)
   - Llamadas a API REST para recoger datos JSON
   - ConexiÃ³n a bases de datos SQL (futuro)

2. **TransformaciÃ³n (Transform)**
   - Limpieza de columnas y filas
   - ConversiÃ³n de tipos de datos
   - Filtrado de outliers
   - PreparaciÃ³n de estructuras limpias

3. **Carga (Load)**
   - InserciÃ³n en tablas PostgreSQL (`raw_data`, `processed_data`)
   - Almacenamiento intermedio en archivos CSV

4. **AutomatizaciÃ³n**
   - Pipeline orquestado con Airflow y DAGs
   - Logs y manejo de errores

5. **ValidaciÃ³n de Datos**
   - Validaciones personalizadas con Great Expectations
   - Pruebas unitarias con PyTest

---

## ğŸ“‚ Estructura del Proyecto

```
etl-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ empleados.csv
â”‚   â”œâ”€â”€ api_data.json
â”‚   â”œâ”€â”€ temp_data.csv
â”‚   â””â”€â”€ empleados_limpios.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ extract_api.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dag_airflow.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Instrucciones para Ejecutar Localmente

### 1. Crear entorno virtual

```bash
python3 -m venv ~/airflow_venv
```

### 2. Activar entorno virtual

```bash
source ~/airflow_venv/bin/activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Ejecutar scripts del pipeline

Primero tenemos activado el entorno virtual: 

source ~/airflow_venv/bin/activate


```bash
cd ~/etl-pipeline/scripts

python extract_api.py       # Extrae datos desde una API REST y guarda un CSV
python extract.py           # Extrae datos desde un CSV/JSON ya existente
python transform.py         # Limpia y transforma los datos
python load.py              # Carga los datos a PostgreSQL



---

## ğŸ“Š Resultados Esperados

- Datos integrados desde mÃºltiples fuentes
- Transformaciones realizadas y validadas
- Tablas `raw_data` y `processed_data` en PostgreSQL
- Carga automatizada vÃ­a Airflow
- Validaciones exitosas con Great Expectations

---

## ğŸ’¡ Siguientes Pasos

ğŸ”¹ AÃ±adir procesamiento con Apache Spark para grandes volÃºmenes  
ğŸ”¹ Desplegar en la nube con Azure o AWS  
ğŸ”¹ Conectar el flujo con dashboards interactivos como Power BI o Grafana  
ğŸ”¹ Incluir control de versiones de datos con Delta Lake

---

ğŸ› ï¸ **Autor:** Carolina Vargas Arce  
ğŸ“… **Fecha de CreaciÃ³n:** Marzo 2025  
ğŸ”— **Repositorio GitHub:** [https://github.com/carolinavarce/carolinavarce](https://github.com/carolinavarce/carolinavarce)
