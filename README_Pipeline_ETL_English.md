# ğŸ“Œ ETL Pipeline with Data Integration and API Extraction

## ğŸ“¢ Description
This project implements an ETL pipeline (Extract, Transform, Load) that gathers data from multiple sources (local files and REST APIs), transforms and validates it, and loads it into a centralized PostgreSQL database. The objective is to build an automated and scalable data pipeline using modern orchestration, storage, and validation tools.

## ğŸ¯ Objectives

âœ… Extract data from:
- REST APIs
- CSV/JSON files
- SQL Databases

âœ… Transform and validate:
- Data cleaning using Pandas
- Check types, nulls, duplicates
- Prepare for structured storage

âœ… Load data into:
- PostgreSQL tables (structured)
- Clean JSON/CSV files

âœ… Automate with Apache Airflow

âœ… Ensure data quality using Great Expectations

---

## ğŸ› ï¸ Tech Stack

- **Language**: Python 3
- **Processing**: Pandas, SQLAlchemy, requests
- **Storage**: PostgreSQL, CSV
- **Validation**: Great Expectations
- **Orchestration**: Apache Airflow
- **Virtual environment**: airflow_venv
- **Version control**: GitHub
- *(Optional)*: Docker, Azure Data Lake

---

## ğŸ—ï¸ Project Architecture

1. **Extraction**
   - Read from CSV files (`empleados.csv`)
   - Call REST APIs to retrieve JSON data
   - (Future) Connect to SQL databases

2. **Transformation**
   - Clean columns and rows
   - Convert data types
   - Filter outliers
   - Prepare final clean structures

3. **Loading**
   - Insert into PostgreSQL tables (`raw_data`, `processed_data`)
   - Intermediate storage in CSV files

4. **Automation**
   - Pipeline orchestrated using Airflow DAGs
   - Logging and error handling

5. **Data Validation**
   - Custom validations using Great Expectations
   - Unit testing with PyTest

---

## ğŸ“‚ Project Structure

```
etl-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ empleados.csv
â”‚   â”œâ”€â”€ api_data.json
â”‚   â”œâ”€â”€ temp_data.csv
â”‚   â””â”€â”€ empleados_limpios.csv
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ extract_api.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dag_airflow.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ How to Run Locally

### 1. Create virtual environment

```bash
python3 -m venv ~/airflow_venv
```

### 2. Activate virtual environment

```bash
source ~/airflow_venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run pipeline scripts

```bash
cd ~/etl-pipeline/scripts

python extract.py         # Extracts from CSV
python extract_api.py     # Extracts from REST API
python transform.py       # Cleans and transforms
python load.py            # Loads into PostgreSQL
```

> You can also orchestrate the entire pipeline using an Airflow DAG if configured.

---

## ğŸ“Š Expected Results

- Data integrated from multiple sources
- Successfully transformed and validated
- `raw_data` and `processed_data` tables populated in PostgreSQL
- Automation enabled via Airflow
- Quality checks passed via Great Expectations

---

## ğŸ’¡ Next Steps

ğŸ”¹ Add Spark for large-scale processing  
ğŸ”¹ Deploy to cloud (Azure or AWS)  
ğŸ”¹ Connect to dashboards (Power BI, Grafana)  
ğŸ”¹ Add data versioning (Delta Lake)

---

ğŸ› ï¸ **Author:** Carolina Vargas Arce  
ğŸ“… **Created:** March 2025  
ğŸ”— **GitHub Repository:** [https://github.com/carolinavarce/carolinavarce](https://github.com/carolinavarce/carolinavarce)
