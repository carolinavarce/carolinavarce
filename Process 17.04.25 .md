

---

## ğŸ“Š Pipeline ETL con Python y Apache Airflow

Este proyecto implementa un pipeline ETL (Extract, Transform, Load) simple utilizando scripts de Python gestionados por Apache Airflow. Actualmente, el flujo de trabajo ya estÃ¡ operativo y visualizable desde la interfaz web de Airflow.

### âœ… Estado actual

El DAG llamado `etl_pipeline` incluye tres tareas principales:

- `extract_data`: Extrae datos desde un archivo local (`empleados.csv`).
- `transform_data`: Realiza transformaciones simples sobre los datos extraÃ­dos.
- `load_data`: Carga o guarda los datos transformados.

Este flujo ya ha sido probado y ejecutado correctamente desde Apache Airflow.

### ğŸ“ Estructura del proyecto

```
etl-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ empleados.csv
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline_dag.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”‚
â””â”€â”€ airflow_venv/
```

### â–¶ï¸ CÃ³mo ejecutar

1. **Activar el entorno virtual**:
   ```bash
   source ~/airflow_venv/bin/activate
   ```

2. **Iniciar la base de datos de Airflow** (solo una vez):
   ```bash
   airflow db migrate
   ```

3. **Ejecutar el scheduler y el servidor web**:
   ```bash
   airflow scheduler
   airflow webserver --port 8080
   ```

4. **Acceder desde el navegador**:
   [http://localhost:8080](http://localhost:8080)

---

## ğŸ› ï¸ PrÃ³ximas mejoras

Ya puedes empezar a incorporar:

- âœ… **Lectura desde una API real** en `extract_api.py`.
- âœ… **Validaciones automÃ¡ticas** con `Great Expectations` o `PyTest`.
- âœ… **ExportaciÃ³n a base de datos** PostgreSQL o SQLite.
- âœ… **ExportaciÃ³n a archivo final `.csv` o `.json`**.
- âœ… **Monitoreo y logging con Airflow o herramientas externas.**

---

## ğŸ“Š ETL Pipeline with Python and Apache Airflow

This project implements a simple ETL (Extract, Transform, Load) pipeline using Python scripts orchestrated by Apache Airflow. The workflow is now running and visualizable from the Airflow web UI.

### âœ… Current Status

The DAG named `etl_pipeline` includes three main tasks:

- `extract_data`: Extracts data from a local file (`empleados.csv`).
- `transform_data`: Applies simple transformations to the extracted data.
- `load_data`: Loads or saves the transformed data.

This workflow has been successfully tested and run inside Apache Airflow.

### ğŸ“ Project structure

```
etl-pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ empleados.csv
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_pipeline_dag.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ load.py
â”‚
â””â”€â”€ airflow_venv/
```

### â–¶ï¸ How to run it

1. **Activate the virtual environment**:
   ```bash
   source ~/airflow_venv/bin/activate
   ```

2. **Initialize the Airflow database** (only once):
   ```bash
   airflow db migrate
   ```

3. **Start the scheduler and webserver**:
   ```bash
   airflow scheduler
   airflow webserver --port 8080
   ```

4. **Access in your browser**:
   [http://localhost:8080](http://localhost:8080)

---

## ğŸ› ï¸ Next Improvements

You can now integrate:

- âœ… **Reading data from a real API** in `extract_api.py`.
- âœ… **Data validation** with `Great Expectations` or `PyTest`.
- âœ… **Data loading into PostgreSQL or SQLite.**
- âœ… **Exporting transformed data to `.csv` or `.json`.**
- âœ… **Monitoring and logging via Airflow or external tools.**
